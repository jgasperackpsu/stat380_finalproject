tp_t <- sum(temp_pred == 1 & test_label_xgb == 1)
fp_t <- sum(temp_pred == 1 & test_label_xgb == 0)
fn_t <- sum(temp_pred == 0 & test_label_xgb == 1)
prec <- tp_t / (tp_t + fp_t)
rec <- tp_t / (tp_t + fn_t)
f1 <- ifelse((prec+rec)==0, 0, 2*prec*rec/(prec+rec))
if(!is.na(f1) && f1 > best_f1){
best_f1 <- f1
best_params <- curr_params
best_nrounds <- cv_res$best_iteration
}
}
}
# I train the final model using the best parameters found by my grid search.
xgb_model <- xgboost(
data = dtrain,
label = train_label_xgb,
params = best_params,
nrounds = best_nrounds,
verbose = 0
)
print("My Best XGBoost Parameters:")
print(unlist(best_params))
# Predict and Tune Threshold
xgb_probs <- predict(xgb_model, dtest)
f1_vec_XGB <- rep(NA, length(thresholds))
# I am optimizing the probability threshold here because "Final Four" is a rare event.
for(i in seq_along(thresholds)){
xgb_pred_loop <- ifelse(xgb_probs > thresholds[i], 1, 0)
cm <- table(Predicted = factor(xgb_pred_loop, levels=c(0,1)),
Actual = factor(test_label_xgb, levels=c(0,1)))
TP <- cm["1","1"]; FP <- cm["1","0"]; FN <- cm["0","1"]
precision_xgb <- ifelse((TP + FP) > 0, TP/(TP+FP), 0)
recall_xgb <- ifelse((TP + FN) > 0, TP/(TP+FN), 0)
f1_vec_XGB[i] <- ifelse((precision_xgb + recall_xgb) > 0,
2 * precision_xgb * recall_xgb / (precision_xgb + recall_xgb), 0)
}
best_index_XGB <- which.max(f1_vec_XGB)
best_threshold_XGB <- thresholds[best_index_XGB]
pred_best_XGB <- ifelse(xgb_probs > best_threshold_XGB, 1, 0)
acc_XGB <- mean(pred_best_XGB == test_label_xgb)
print(paste("My Best XGBoost F1 Score:", max(f1_vec_XGB)))
#| label: xgboostFeatureImp
# Feature Importance for XGBoost
importance_matrix <- xgb.importance(colnames(dtrain), model = xgb_model)
xgb.plot.importance(importance_matrix, top_n = 10, main = "XGBoost Feature Importance")
#| label: svmModel
# I implemented an SVM to satisfy the "multiple machine learning techniques" requirement.
# I chose a 'radial' kernel to allow for non-linear decision boundaries.
svm_model <- svm(
isFinalFour ~ TPR + TPRD + W + ADJOE + ADJDE + TOR + TORD + FTR,
data = train_RF,
type = 'C-classification',
kernel = 'radial',
probability = TRUE
)
# I needed to enable probability=TRUE above so I could extract probabilities
# for the ROC curve comparison later.
svm_probs_obj <- predict(svm_model, newdata = test_RF, probability = TRUE)
svm_probs <- attr(svm_probs_obj, "probabilities")[, "1"]
# Tuning the threshold for SVM (Same logic as my XGBoost tuning)
f1_vec_SVM <- rep(NA, length(thresholds))
for(i in seq_along(thresholds)){
svm_pred_loop <- ifelse(svm_probs > thresholds[i], 1, 0)
cm <- table(Predicted = factor(svm_pred_loop, levels=c(0,1)),
Actual = test_RF$isFinalFour)
TP <- cm["1","1"]; FP <- cm["1","0"]; FN <- cm["0","1"]
precision_svm <- ifelse((TP + FP) > 0, TP/(TP+FP), 0)
recall_svm <- ifelse((TP + FN) > 0, TP/(TP+FN), 0)
f1_vec_SVM[i] <- ifelse((precision_svm + recall_svm) > 0,
2 * precision_svm * recall_svm / (precision_svm + recall_svm), 0)
}
best_index_SVM <- which.max(f1_vec_SVM)
best_threshold_SVM <- thresholds[best_index_SVM]
pred_best_SVM <- ifelse(svm_probs > best_threshold_SVM, 1, 0)
acc_SVM <- mean(pred_best_SVM == as.numeric(test_RF$isFinalFour) - 1)
print(paste("My Best SVM F1 Score:", max(f1_vec_SVM)))
#| labels: modelROCCurves
# I aggregated all our models here to visually compare their performance.
# This ROC plot allows us to see which model maintains the best True Positive Rate
# as the False Positive Rate increases.
# ROC Curves
roc_knn <- roc(y_test, knn_probs)
roc_nn  <- roc(y_test_NN, nn_probs)
roc_rf  <- roc(y_test_RF, rf_probs)
roc_xgb <- roc(test_label_xgb, xgb_probs)
roc_svm <- roc(test_RF$isFinalFour, svm_probs)
plot(roc_rf, col="blue", main="ROC Curves Comparison (All Models)")
lines(roc_xgb, col="purple")
lines(roc_svm, col="orange")
lines(roc_knn, col="red")
lines(roc_nn, col="green")
legend("bottomright",
legend=c("RF", "XGBoost (Bonus)", "SVM (Addition)", "kNN", "NN"),
col=c("blue", "purple", "orange", "red", "green"),
lwd=2)
# I calculated the AUC for each to give us a single metric for comparison.
auc_knn <- auc(roc_knn)
auc_nn <- auc(roc_nn)
auc_rf <- auc(roc_rf)
auc_xgb <- auc(roc_xgb)
auc_svm <- auc(roc_svm)
print(paste("AUC of K-Nearest Neighbors:", round(auc_knn,3)))
print(paste("AUC of Neural Network:", round(auc_nn,3)))
print(paste("AUC of Random Forest:", round(auc_rf,3)))
print(paste("AUC of XGBoost (Mine):", round(auc_xgb,3)))
print(paste("AUC of SVM (Mine):", round(auc_svm,3)))
#| label: balancedAccuracy
# I am calculating Balanced Accuracy manually for all models here to compare them fairly.
knn_cm <- table(Predicted = knn_model, Actual = y_test)
nn_cm <- table(Predicted = pred_best, Actual = y_test_NN)
rf_cm <- table(Predicted = pred_best_RF, Actual = y_test_RF)
xgb_cm <- table(Predicted = factor(pred_best_XGB, levels=c(0,1)), Actual = factor(test_label_xgb, levels=c(0,1)))
svm_cm <- table(Predicted = factor(pred_best_SVM, levels=c(0,1)), Actual = test_RF$isFinalFour)
# True positives
TP_knn <- ifelse("1" %in% rownames(knn_cm) & "1" %in% colnames(knn_cm), knn_cm["1","1"], 0)
TP_nn <- ifelse("1" %in% rownames(nn_cm) & "1" %in% colnames(nn_cm), nn_cm["1","1"], 0)
TP_rf <- ifelse("1" %in% rownames(rf_cm) & "1" %in% colnames(rf_cm), rf_cm["1","1"], 0)
TP_xgb <- ifelse("1" %in% rownames(xgb_cm) & "1" %in% colnames(xgb_cm), xgb_cm["1","1"], 0)
TP_svm <- ifelse("1" %in% rownames(svm_cm) & "1" %in% colnames(svm_cm), svm_cm["1","1"], 0)
# False Positives
FP_knn <- ifelse("1" %in% rownames(knn_cm) & "0" %in% colnames(knn_cm), knn_cm["1","0"], 0)
FP_nn <- ifelse("1" %in% rownames(nn_cm) & "0" %in% colnames(nn_cm), nn_cm["1","0"], 0)
FP_rf <- ifelse("1" %in% rownames(rf_cm) & "0" %in% colnames(rf_cm), rf_cm["1","0"], 0)
FP_xgb <- ifelse("1" %in% rownames(xgb_cm) & "0" %in% colnames(xgb_cm), xgb_cm["1","0"], 0)
FP_svm <- ifelse("1" %in% rownames(svm_cm) & "0" %in% colnames(svm_cm), svm_cm["1","0"], 0)
# False Negatives
FN_knn <- ifelse("0" %in% rownames(knn_cm) & "1" %in% colnames(knn_cm), knn_cm["0","1"], 0)
FN_nn <- ifelse("0" %in% rownames(nn_cm) & "1" %in% colnames(nn_cm), nn_cm["0","1"], 0)
FN_rf <- ifelse("0" %in% rownames(rf_cm) & "1" %in% colnames(rf_cm), rf_cm["0","1"], 0)
FN_xgb <- ifelse("0" %in% rownames(xgb_cm) & "1" %in% colnames(xgb_cm), xgb_cm["0","1"], 0)
FN_svm <- ifelse("0" %in% rownames(svm_cm) & "1" %in% colnames(svm_cm), svm_cm["0","1"], 0)
# True Negatives
TN_knn <- ifelse("0" %in% rownames(knn_cm) & "0" %in% colnames(knn_cm), knn_cm["0","0"], 0)
TN_nn <- ifelse("0" %in% rownames(nn_cm) & "0" %in% colnames(nn_cm), nn_cm["0","0"], 0)
TN_rf <- ifelse("0" %in% rownames(rf_cm) & "0" %in% colnames(rf_cm), rf_cm["0","0"], 0)
TN_xgb <- ifelse("0" %in% rownames(xgb_cm) & "0" %in% colnames(xgb_cm), xgb_cm["0","0"], 0)
TN_svm <- ifelse("0" %in% rownames(svm_cm) & "0" %in% colnames(svm_cm), svm_cm["0","0"], 0)
# Recalls and Specificities
recall_knn <- ifelse((TP_knn + FN_knn) > 0, TP_knn/(TP_knn + FN_knn), 0)
recall_nn <- ifelse((TP_nn + FN_nn) > 0, TP_nn/(TP_nn + FN_nn), 0)
recall_rf <- ifelse((TP_rf + FN_rf) > 0, TP_rf/(TP_rf + FN_rf), 0)
recall_xgb <- ifelse((TP_xgb + FN_xgb) > 0, TP_xgb/(TP_xgb + FN_xgb), 0)
recall_svm <- ifelse((TP_svm + FN_svm) > 0, TP_svm/(TP_svm + FN_svm), 0)
spec_knn <- ifelse((TN_knn + FP_knn) > 0, TN_knn/(TN_knn + FP_knn), 0)
spec_nn <- ifelse((TN_nn + FP_nn) > 0, TN_nn/(TN_nn + FP_nn), 0)
spec_rf <- ifelse((TN_rf + FP_rf) > 0, TN_rf/(TN_rf + FP_rf), 0)
spec_xgb <- ifelse((TN_xgb + FP_xgb) > 0, TN_xgb/(TN_xgb + FP_xgb), 0)
spec_svm <- ifelse((TN_svm + FP_svm) > 0, TN_svm/(TN_svm + FP_svm), 0)
# Balanced Accuracies
BA_knn <- (recall_knn + spec_knn)/2
BA_nn <- (recall_nn + spec_nn)/2
BA_rf <- (recall_rf + spec_rf)/2
BA_xgb <- (recall_xgb + spec_xgb)/2
BA_svm <- (recall_svm + spec_svm)/2
#| label: evaluationMetrics
rf_metrics <- list(
"F1 Score" = round(f1_vec_RF[best_index_RF], 3),
Accuracy = round(acc_RF, 3),
"Area Under Curve" = round(auc_rf, 3),
"Balanced Accuracy" = round(BA_rf, 3)
)
xgb_metrics <- list(
"F1 Score" = round(max(f1_vec_XGB), 3),
Accuracy = round(acc_XGB, 3),
"Area Under Curve" = round(auc_xgb, 3),
"Balanced Accuracy" = round(BA_xgb, 3)
)
svm_metrics <- list(
"F1 Score" = round(max(f1_vec_SVM), 3),
Accuracy = round(acc_SVM, 3),
"Area Under Curve" = round(auc_svm, 3),
"Balanced Accuracy" = round(BA_svm, 3)
)
knn_metrics <- list(
"F1 Score" = round(f1_vec[best_k], 3),
Accuracy = round(acc_k3, 3),
"Area Under Curve" = round(auc_knn, 3),
"Balanced Accuracy" = round(BA_knn, 3)
)
nn_metrics <- list(
"F1 Score" = round(f1_vec_NN[best_index], 3),
Accuracy = round(acc_NN, 3),
"Area Under Curve" = round(auc_nn, 3),
"Balanced Accuracy" = round(BA_nn, 3)
)
print("--- Random Forest ---")
print(rf_metrics)
print("--- XGBoost (Bonus) ---")
print(xgb_metrics)
print("--- SVM ---")
print(svm_metrics)
print("--- kNN ---")
print(knn_metrics)
print("--- Neural Network ---")
print(nn_metrics)
#| label: predict2025
cbb25_test <- cbb25 %>%
rename(
"TPR" = `2P_O`,
"TPRD" = `2P_D`
) %>%
select(Team, all_of(features_RF))
# Here we can stick with the RF model for the final visual prediction as it is robust,
# but I can easily swap 'rf_model' with 'xgb_model' here if XGBoost performed better.
probs_2025 <- predict(rf_model,
newdata = cbb25_test,
type = "prob")[,2]
pred_2025 <- ifelse(probs_2025 > thresholds[best_index_RF], 1, 0)
for(i in seq_along(probs_2025)){
names(probs_2025) <- cbb25_test$Team
}
predictions <- data.frame(
probs = probs_2025,
preds = pred_2025,
Team = cbb25_test$Team
) %>%
arrange(desc(probs))
predictedFinalFours <- predictions %>%
filter(preds == 1) %>%
mutate(probs = round(probs,2)) %>%
arrange(desc(probs))
top4Teams <- predictions %>%
top_n(4, probs)
## Plot Top 4 teams and 4 honorable mentions
ggplot(
data = predictedFinalFours,
mapping = aes(x = reorder(Team, -probs), y = probs)
) +
geom_col(color = "black", fill = "skyblue") +
geom_text(aes(label = probs),
vjust = -0.3, size = 3)+
scale_y_continuous(limits = c(0,1)) +
labs(title = "Teams and Their Probability to Make the Final Four",
x = "Team",
y = "Probability") +
theme_bw() +
theme(plot.title = element_text(face = "bold", hjust = 0.5),
axis.title = element_text(face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1))
knitr::opts_chunk$set(echo = TRUE)
#| label: frontMatter
#| result: 'hide'
# Add libraries
library(tidyverse)
library(ggplot2)
library(stringr)
# Load in data files
zip_raw <- "https://codeload.github.com/jgasperackpsu/stat380_finalproject/zip/refs/heads/main"
temp <- tempfile(fileext = ".zip")
temp_dir = tempdir()
download.file(zip_raw, temp)
unzip(temp, exdir = temp_dir)
csv_files <- list.files(temp_dir, pattern = "\\.csv$", full.names = TRUE,
recursive = TRUE)
# Convert files to datasets
datasets <- lapply(csv_files, read_csv)
names(datasets) <- tools::file_path_sans_ext(basename(csv_files))
names <- rep(NA, length(datasets))
for(i in seq_along(datasets)){
names[i] <- names(datasets[i])
}
for(name in names){
assign(name, datasets[[name]])
}
#| label: cleaningTypes
# Some datasets have character integers
cleanSEEDS <- function(seed){
ifelse(seed == "N/A", NA, as.numeric(seed))
}
cbb22$SEED <- sapply(cbb22$SEED, cleanSEEDS)
cbb23$SEED <- sapply(cbb23$SEED, cleanSEEDS)
cbb24$SEED <- sapply(cbb24$SEED, cleanSEEDS)
cbb25$SEED <- sapply(cbb25$SEED, cleanSEEDS)
cbb$SEED <- sapply(cbb$SEED, cleanSEEDS)
print(str(cbb25))
unique_vals <- sapply(cbb25, function(x) length(unique(x)))
print(unique_vals)
#| label: sameVars
# 2020 no postseason -> COVID
# 2025 no postseason -> Season just began
# All other datasets should have same number variables and '20,'25 have 1 less
cbb25 <- cbb25 %>% select(-c("RK", "3PR", "3PRD"))
# Remove stats that only appear in '25
# Remove rank as there's a seed column
# Keep rank in 2020 since there's no Seed column
# Make col names the same
cbb24 <- cbb24 %>% rename(EFG_O = `EFG%`,  EFG_D = `EFGD%`)
cbb22 <- cbb22 %>% rename(EFG_D = EFGD_D)
#| label: offensiveDist
# Finding the distributions of our offensive stats
off_stats <- c("ADJOE","EFG_O","TOR","ORB","FTR","2P_O","3P_O")
nice_names <- c(
"ADJOE"="Adjusted Offensive Efficiency",
"EFG_O"="Effective Field Goal Rate",
"TOR"="Turnover Rate",
"ORB"="Offensive Rebound Rate",
"FTR"="Free Throw Rate",
"2P_O"="2-Point FG Rate",
"3P_O"="3-Point FG Rate"
)
stats_since2013 <- cbb %>%
group_by(TEAM) %>%
summarize(
ADJOE = mean(ADJOE),
ADJDE = mean(ADJDE),
EFG_O = mean(EFG_O),
EFG_D = mean(EFG_D),
TOR = mean(TOR),
TORD = mean(TORD),
ORB = mean(ORB),
DRB = mean (DRB),
FTR = mean(FTR),
FTRD = mean(FTRD),
`2P_O` = mean(`2P_O`),
`2P_D` = mean(`2P_D`),
`3P_O` = mean(`3P_O`),
`3P_D` = mean(`3P_D`)
) %>%
mutate(across(where(is.numeric), ~round(.x,2)))
for(stat in off_stats){
print(
ggplot(data = stats_since2013, mapping = aes(x = .data[[stat]])) +
geom_histogram(fill = "lightblue", color = "black") +
labs(title = paste("Distribution of",nice_names[stat]),
x = nice_names[stat],
y = "Count") +
theme_minimal() +
theme(plot.title = element_text(face = "bold", hjust = 0.5),
axis.title = element_text(face = "bold"))
)
}
#| label: defensiveDist
def_stats <- c("ADJDE","EFG_D","TORD","DRB","FTRD","2P_D","3P_D")
nice_names_def <- c(
"ADJDE"="Adjusted Defensive Efficiency",
"EFG_D"="Effective Field Goal Rate Allowed",
"TORD"="Turnover Rate Allowed",
"DRB"="Offensive Rebound Rate Allowed",
"FTRD"="Free Throw Rate Allowed",
"2P_D"="2-Point FG Rate Allowed",
"3P_D"="3-Point FG Rate Allowed"
)
for(stat in def_stats){
print(
ggplot(data = stats_since2013, mapping = aes(x = .data[[stat]])) +
geom_histogram(fill = "lightblue", color = "black") +
labs(title = paste("Distribution of",nice_names_def[stat]),
x = nice_names_def[stat],
y = "Count") +
theme_minimal() +
theme(plot.title = element_text(face = "bold", hjust = 0.5),
axis.title = element_text(face = "bold"))
)
}
#| label: seedProduction
seed_game_production <- cbb %>%
filter(!is.na(SEED)) %>%
group_by(TEAM, YEAR, SEED) %>%
summarize(
ADJOE = mean(ADJOE),
ADJDE = mean(ADJDE)
)
nrow(seed_game_production)
sums <- rep(NA,16) # Check that there are 4 of each seed per year
for(i in seq_along(sums)){
sums[i] <- sum(seed_game_production==i)
}
sumsperyear <- rep(NA,16) # There isn't so we have to fix that
for(i in seq_along(sumsperyear)){
sumsperyear[i] <- list(table(filter(seed_game_production, SEED == i)$YEAR))
}
#| label: correctSeedings
cbb <- cbb %>%
mutate(SEED = case_when(
(TEAM == "Iowa St.") & (YEAR == 2016) ~ 4L,
(TEAM == "George Washington") & (YEAR == 2014) ~ 9L,
TRUE ~ SEED
),
POSTSEASON = case_when(
POSTSEASON == "R68" ~ "R64",
POSTSEASON == "N/A" ~ NA,
TRUE ~ POSTSEASON
)
)
# There are 6 16 seeds for every year and I found there are 2 other teams of low
# seedings as well, those teams are the first four teams that got beat in the games
# leading up to the march madness so the winners secure their spot in the actual
# tournament
# So instead of removing those teams I'll be naming them a new integer -1
first4_2013 <- c("Middle Tennessee", "Boise St.", "LIU Brooklyn", "Liberty")
first4_2014 <- c("Iowa", "Xavier", "Texas Southern", "Mount St. Mary's")
first4_2015 <- c("Boise St.","BYU","North Florida", "Manhattan")
first4_2016 <- c("Vanderbilt", "Tulsa","Fairleigh Dickinson", "Southern")
first4_2017 <-c("Wake Forest","North Carolina Central","Providence","New Orleans")
first4_2018 <- c("UCLA","Arizona St.","North Carolina Central","LIU Brooklyn")
first4_2019 <- c("Temple","St. John's","Prairie View A&M","North Carolina Central")
first4_2021 <- c("Michigan St.","Wichita St.","Mount St. Mary's","Appalachian St.")
first4_2022 <- c("Rutgers","Wyoming","Texas A&M Corpus Chris","Bryant")
first4_2023 <- c("Nevada","Mississippi St.","Texas Southern","Southeast Missouri St.")
first4_2024 <- c("Virginia","Boise St.","Howard","Montana St.")
cbb <- cbb %>%
mutate(SEED = case_when(
(TEAM %in% first4_2013) & (YEAR == 2013) ~ -1,
(TEAM %in% first4_2014) & (YEAR == 2014) ~ -1,
(TEAM %in% first4_2015) & (YEAR == 2015) ~ -1,
(TEAM %in% first4_2016) & (YEAR == 2016) ~ -1,
(TEAM %in% first4_2017) & (YEAR == 2017) ~ -1,
(TEAM %in% first4_2018) & (YEAR == 2018) ~ -1,
(TEAM %in% first4_2019) & (YEAR == 2019) ~ -1,
(TEAM %in% first4_2021) & (YEAR == 2021) ~ -1,
(TEAM %in% first4_2022) & (YEAR == 2022) ~ -1,
(TEAM %in% first4_2023) & (YEAR == 2023) ~ -1,
(TEAM %in% first4_2024) & (YEAR == 2024) ~ -1,
TRUE ~ SEED
))
new_seed_game_production <- cbb %>%
filter(!is.na(SEED)) %>%
group_by(TEAM, YEAR, SEED) %>%
summarize(
ADJOE = mean(ADJOE),
ADJDE = mean(ADJDE)
)
new_sumsperyear <- rep(NA,17) # New check
for(i in seq_along(new_sumsperyear)){
new_sumsperyear[i] <- list(table(filter(new_seed_game_production, SEED ==
ifelse(i==17,-1,i))$YEAR))
}
#| label: seedVSefficiency
top4 <- new_seed_game_production %>%
filter(SEED <= 4 & SEED > 0)
bottom4 <- new_seed_game_production %>%
filter(SEED > 12)
best_and_worst <- bind_rows(top4, bottom4)
# Using best group and worst group since the difference between the one seed and the
# 16th seed is smaller than most people think, and since showing all data points is
# too visually crowding, we compare the "worst" teams to the "best" teams
ggplot(
data = top4,
mapping = aes(x = ADJOE, y = ADJDE, color = factor(SEED))
) +
geom_point(alpha = 0.75, size = 1.5) +
geom_smooth(linewidth = .8, color = "skyblue", se = FALSE) +
labs(title = "Offensive vs. Defensive Efficiency by Seeding",
x = "Offensive Efficiency",
y = "Defensive Efficiency",
color = "Seeding") +
scale_color_manual(values = c("1" = "lightgreen", "2" = "yellowgreen", "3" =
"orange", "4" = "orangered")) +
theme_minimal() +
theme(plot.title = element_text(face = "bold", hjust = 0.5),
axis.title = element_text(face = "bold"),
legend.title = element_text(face = "bold", hjust = 0.5))
ggplot(
data = best_and_worst,
mapping = aes(x = ADJOE, y = ADJDE, color = factor(SEED))
) +
geom_point(alpha = 0.75, size = 3) +
geom_smooth(linewidth = .8, color = "skyblue", se = FALSE) +
labs(title = "Offensive vs. Defensive Efficiency by Seeding",
x = "Offensive Efficiency",
y = "Defensive Efficiency",
color = "Seeding") +
scale_color_manual(values = c("1" = "lightgreen", "2" = "green", "3" = "greenyellow",
"4" = "yellow", "13" = "orange", "14" = "orangered", "15" = "red",
"16" = "darkred")) +
theme_minimal() +
theme(plot.title = element_text(face = "bold", hjust = 0.5),
axis.title = element_text(face = "bold"),
legend.title = element_text(face = "bold", hjust = 0.5))
#| label: productionContributors
off_contrib <- c("EFG_O","TOR","ORB","FTR","2P_O","3P_O")
def_contrib <- c("EFG_D","TORD","DRB","FTRD","2P_D","3P_D")
off_corrs <- rep(NA, length(off_contrib))
def_corrs <- rep(NA, length(def_contrib))
for(i in seq_along(off_contrib)){
stat <- off_contrib[i]
off_corrs[i] <- cor(cbb$ADJOE, cbb[[stat]], use = "complete.obs")
}
off_corrs
for(i in seq_along(def_contrib)){
stat <- def_contrib[i]
def_corrs[i] <- cor(cbb$ADJDE, cbb[[stat]], use = "complete.obs")
}
def_corrs
#| label: postseasonStats
postseasonRanks <- cbb %>%
group_by(POSTSEASON) %>%
summarize(
ADJOE = mean(ADJOE),
ADJDE = mean(ADJDE),
) %>%
mutate(POSTSEASON =
factor(POSTSEASON, levels = c("Champions", "2ND", "F4", "E8", "S16", "R32", "R64"))
)
ggplot(
data = postseasonRanks,
mapping = aes(x = ADJOE, y = ADJDE, color = POSTSEASON)
) +
geom_point(alpha = 0.75, size = 3.5) +
geom_smooth(linewidth = .8, color = "steelblue", se = FALSE) +
labs(title = "Offensive vs. Defensive Efficiency by Postseason Results",
x = "Offensive Efficiency",
y = "Defensive Efficiency",
color = "Postseason Results") +
scale_color_manual(values = c("Champions" = "gold", "2ND" = "gray", "F4" ="brown",
"E8" = "lightblue", "R16" = "blue","R32" = "darkblue",
"R64"= "navy"), na.value = "black") +
theme_minimal() +
theme(plot.title = element_text(face = "bold", hjust = 0.5),
axis.title = element_text(face = "bold"),
legend.title = element_text(face = "bold", hjust = 0.5))
