---
title: "Final Project ML Model Creation"
author: "Jackson Gasperack"
date: "2025-11-19"
format:
  pdf:
    toc: true
---

```{r}
load(file = "EDA.RData")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Front Matter
```{r}
#| label: frontMatter

# Add libraries
library(tidyverse)
library(ggplot2)
library(caret)
library(glmnet)
library(randomForest)
library(FNN)
library(pROC)
library(nnet)
library(PRROC)

```

Mentioned in class:

Models:
Simple/Multiple Regression, Logistic Regression, Binomial Regression, k-Nearest Neighbors (Regression/Classification), Na√Øve Bayes, SVM, Spline Regression, Neural Networks

Model Evaluations:
RMSE, k-fold Cross Validation, Confusion Matrix (Accuracy, Recall, ...), ROC Curves, AUC, R-squared, Subset Selection (Forward, Backward), Shrinkage (LASSO, Ridge), AIC, BIC, MAE, Bootstrap Validation

NOT Mentioned in class:

Models:
RandomForest, XGBoost, Poisson/Gamma Regression, t-SNE, PCA, k-Means

Model Evaluations:
Precision-Recall Curve, F1 Score, MAPE

## Question

Can we use the stats of the last 12 years to predict who will be in the final four for March Madness?

## New variable

```{r}
#| label: newVar

cbb <- cbb %>%
  mutate(
    isFinalFour = ifelse(POSTSEASON %in% c("Champions","2ND","F4"), 1, 0),
    isFinalFour = ifelse(is.na(POSTSEASON), 0, isFinalFour),
    POSTSEASON = ifelse(is.na(POSTSEASON), "No Playoffs", POSTSEASON),
    SEED = ifelse(is.na(SEED), "No Seeding", SEED)
  )
```


## Find best feature set

```{r}
#| label: featureSelection
#| results: 'hide'

# Backwards subset selection
full_model <- glm(isFinalFour ~ CONF + W + ADJOE + ADJDE + BARTHAG + EFG_O +
                   EFG_D + TOR + TORD + ORB + DRB + FTR + FTRD + `2P_O` +
                   `2P_D` + `3P_O` + `3P_D` + ADJ_T + SEED,
                 data = cbb,
                 family = binomial)

null_model <- glm(isFinalFour ~ 1, data = cbb, family = binomial)

forward_selection <- step(null_model, 
                          scope = list(lower = null_model,
                                       upper = full_model),
                          direction = "forward")

backward_selection <- step(full_model, direction = "backward")

stepwise_selection <- step(full_model, direction = "both")

print(summary(forward_selection))
print(summary(backward_selection))
print(summary(stepwise_selection))
```
Forward model is the one we'll move forward with, lowest AIC.
X = W, ADJOE, TOR, FTR, 2P_D, 2P_O, and BARTHAG but since it's not statistically significant we will try and replace it with some other variables

```{r}
#| label: modelTesting

model1 <- glm(isFinalFour ~ W + ADJOE + TOR + FTR + `2P_D` + `2P_O` + ADJDE + TORD,
              data = cbb,
              family = "binomial")

summary(model1)
```
Found model1 adds two more predictors to reduce underfitting while also decreasing AIC

## kNN

```{r}
#| label: kNearestNeighbors

features <- c("W","ADJOE","ADJDE","2P_O","2P_D","TOR","TORD","FTR")
max_k <- 30
num_folds <- 10
folds <- cut(1:nrow(cbb), breaks = num_folds, labels = FALSE)

f1_vec <- rep(NA, max_k) # w/ rare successes just accuracy is misleading

set.seed(1234) # Reproducibility
folds <- sample(folds) # k-fold CV on sample
set.seed(NULL)

# Train and test data
for(i in 1:max_k){
  fold_f1 <- rep(NA,num_folds)
  
    for(j in 1:num_folds){
    test_ind <- which(folds == j)
    train_data <- cbb[-test_ind,c(features,"isFinalFour")]
    test_data <- cbb[test_ind,c(features,"isFinalFour")]
    
    X_train <- train_data[,features]
    X_test <- test_data[,features]
    y_train <- train_data$isFinalFour
    y_test <- test_data$isFinalFour
    
    # Scale features
    X_train_scaled <- scale(X_train)
    X_test_scaled <- scale(X_test, center = attr(X_train_scaled, "scaled:center"),
                                   scale = attr(X_train_scaled, "scaled:scale"))
    
    # kNN classifier 
    pred <- knn(X_train_scaled, X_test_scaled, cl = y_train, k = i)
    cm <- table(Predicted = pred, Actual = y_test)
    
    TP <- ifelse("1" %in% rownames(cm) & "1" %in% colnames(cm), cm["1","1"], 0)
    FP <- ifelse("1" %in% rownames(cm) & "0" %in% colnames(cm), cm["1","0"], 0)
    FN <- ifelse("0" %in% rownames(cm) & "1" %in% colnames(cm), cm["0","1"], 0)
  
    precision <- ifelse((TP + FP) > 0, TP/(TP+FP), 0)
    recall <- ifelse((TP + FN) > 0, TP/(TP+FN), 0)
    fold_f1[j] <- ifelse((precision + recall) > 0, 
                       2 * precision * recall / (precision + recall),
                       0)
    }
  
  f1_vec[i] <- mean(fold_f1)
}

best_k = which.max(f1_vec)
best_k
f1_vec[best_k]

# Find accuracy of k = 3
knn_model <- knn(X_train_scaled, X_test_scaled, cl = y_train, k = 3, prob = TRUE)
knn_probs <- attr(knn_model, "prob")  # probability of the predicted class
acc_k3 <- mean(knn_model == y_test)
acc_k3
```

```{r}
#| label: f1VSk

f1_k_df <- data.frame(
  k=1:max_k,
  f1=f1_vec
)

ggplot(data = f1_k_df, mapping = aes(x = k, y = f1)) +
  geom_line() +
  geom_point(alpha = 0.8, color = "red") +
  labs(title = "F1 Score vs. k-Value",
       subtitle = "F1 = (2*Recall*Precision)/(Recall + Precision)",
       x = "k-Value for kNN model",
       y = "F1 Score") +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", hjust = 0.5, size = 14),
        plot.subtitle = element_text(face = "italic", hjust = 0.5, size = 10),
        axis.title = element_text(face = "bold")
        )
```

## Neural Networks

```{r}
#| label: neuralNetwork

# This time we won't use k-fold for our sampling

set.seed(1234)
train_inds_NN <- sample(1:nrow(cbb),.85*nrow(cbb)) # 85/15 split
set.seed(NULL)

train_NN <- cbb[train_inds_NN,c(features,"isFinalFour")]
test_NN <- cbb[-train_inds_NN,c(features,"isFinalFour")]

# split into x and y
X_train_NN <- scale(train_NN[,features])
X_test_NN <- scale(test_NN[,features], 
                center = attr(X_train_NN, "scaled:center"),
                scale = attr(X_train_NN, "scaled:scale"))

y_train_NN <- train_NN$isFinalFour
y_test_NN <- test_NN$isFinalFour

# Create model

nn_model <- nnet(
  x = X_train_NN,
  y = y_train_NN,
  size = 5,
  maxit = 1000,
  linout = F,
  entropy = T
)
```

```{r}
#| label: neuralEval

# NN evaluations
nn_probs <- predict(nn_model, X_test_NN)[,1]

# do k-fold for best possible threshold from 0.001-0.15 since 4/350 teams make
# final 4 every year
thresholds <- seq(0.001, 0.15, by = 0.001)
f1_vec_NN <- rep(NA,length(thresholds))
for(i in seq_along(thresholds)){
  nn_pred <- ifelse(nn_probs > thresholds[i], 1, 0)
  cm <- table(Predicted = nn_pred, Actual = y_test_NN)
  
  TP <- ifelse("1" %in% rownames(cm) & "1" %in% colnames(cm), cm["1","1"], 0)
  FP <- ifelse("1" %in% rownames(cm) & "0" %in% colnames(cm), cm["1","0"], 0)
  FN <- ifelse("0" %in% rownames(cm) & "1" %in% colnames(cm), cm["0","1"], 0)

  precision_NN <- ifelse((TP + FP) > 0, TP/(TP+FP), 0)
  recall_NN <- ifelse((TP + FN) > 0, TP/(TP+FN), 0)
  f1_vec_NN[i] <- 2*precision_NN*recall_NN/(precision_NN + recall_NN)
}

best_index <- which.max(f1_vec_NN)
thresholds[best_index]
f1_vec_NN[best_index]

# Find accuracy of best threshold
pred_best <- ifelse(nn_probs > thresholds[best_index], 1, 0)
acc_NN <- mean(pred_best == y_test_NN)
acc_NN
```


```{r}
#| label: thresholdVSF1

f1_threshold_df <- data.frame(
  threshold=thresholds,
  f1=f1_vec_NN
)

ggplot(data = f1_threshold_df, mapping = aes(x = threshold, y = f1)) +
  geom_point(alpha = 0.8, color = "darkgreen") +
  labs(title = "F1 Score vs. Threshold",
       subtitle = "F1 = (2*Recall*Precision)/(Recall + Precision)",
       x = "Threshold for Neural Network Model",
       y = "F1 Score") +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", hjust = 0.5, size = 14),
        plot.subtitle = element_text(face = "italic", hjust = 0.5, size = 10),
        axis.title = element_text(face = "bold")
        )
```

## Random Forest
```{r}
#| label: randomForest

set.seed(1234)
train_inds_RF <- sample(1:nrow(cbb),.85*nrow(cbb))
set.seed(NULL)

features_RF <- c("W","ADJOE","ADJDE","TPR","TPRD","TOR","TORD","FTR")
# Note TPR and TPRD is 2P_O/D but randomForest model was having trouble parsing
# backticks and variables that start with integer

train_RF <- cbb[train_inds_RF,c(features,"isFinalFour")]
test_RF <- cbb[-train_inds_RF,c(features,"isFinalFour")]

train_RF <- train_RF %>%
  rename(
    "TPR" = `2P_O`, # Two-point rate
    "TPRD" = `2P_D`
  ) %>%
  mutate(isFinalFour = factor(isFinalFour, levels = c(0,1)))
test_RF <- test_RF %>%
  rename(
    "TPR" = `2P_O`,
    "TPRD" = `2P_D`
  ) %>%
  mutate(isFinalFour = factor(isFinalFour, levels = c(0,1)))

X_train_RF <- scale(train_RF[,features_RF])
X_test_RF <- scale(test_RF[,features_RF], 
                   center = attr(X_train_RF, "scaled:center"),
                   scale = attr(X_train_RF, "scaled:scale"))

y_train_RF <- train_RF$isFinalFour
y_test_RF <- test_RF$isFinalFour

rf_model <- randomForest(
  isFinalFour ~ TPR + TPRD + W + ADJOE + ADJDE + TOR + TORD + FTR,
  data = train_RF,
  ntree = 5000,
  mtry = floor(sqrt(ncol(train_RF) - 1)),
  importance = T
)

rf_model
```

```{r}
#| label: rfEval

# Calculate predicted probabilities
rf_probs <- predict(rf_model, newdata = test_RF, type = "prob")[,2]

# Use same thresholds vector
f1_vec_RF <- rep(NA,length(thresholds))
for(i in seq_along(thresholds)){
  rf_pred <- ifelse(rf_probs > thresholds[i], 1, 0)
  cm <- table(Predicted = rf_pred, Actual = y_test_RF)
  
  TP <- ifelse("1" %in% rownames(cm) & "1" %in% colnames(cm), cm["1","1"], 0)
  FP <- ifelse("1" %in% rownames(cm) & "0" %in% colnames(cm), cm["1","0"], 0)
  FN <- ifelse("0" %in% rownames(cm) & "1" %in% colnames(cm), cm["0","1"], 0)

  precision_RF <- ifelse((TP + FP) > 0, TP/(TP+FP), 0)
  recall_RF <- ifelse((TP + FN) > 0, TP/(TP+FN), 0)
  f1_vec_RF[i] <- 2*precision_RF*recall_RF/(precision_RF + recall_RF)
}

best_index_RF <- which.max(f1_vec_RF)
thresholds[best_index_RF]
f1_vec_RF[best_index_RF]

# Find accuracy of best threshold
pred_best_RF <- ifelse(rf_probs > thresholds[best_index_RF], 1, 0)
acc_RF <- mean(pred_best_RF == y_test_RF)
acc_RF
```

```{r}
#| label: thresholdVSF1_RF

f1_threshold_RF_df <- data.frame(
  threshold=thresholds,
  f1=f1_vec_RF
)

ggplot(data = f1_threshold_RF_df, mapping = aes(x = threshold, y = f1)) +
  geom_point(alpha = 0.8, color = "blue") +
  labs(title = "F1 Score vs. Threshold",
       subtitle = "F1 = (2*Recall*Precision)/(Recall + Precision)",
       x = "Threshold for Random Forest Model",
       y = "F1 Score") +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", hjust = 0.5, size = 14),
        plot.subtitle = element_text(face = "italic", hjust = 0.5, size = 10),
        axis.title = element_text(face = "bold")
        )
```


## Model Comparison

```{r}
#| labels: modelROCCurves

# ROC Curves
roc_knn <- roc(y_test, knn_probs)
roc_nn  <- roc(y_test_NN, nn_probs)
roc_rf  <- roc(y_test_RF, rf_probs)

plot(roc_knn, col="red", main="ROC Curves Comparison")
lines(roc_nn, col="green")
lines(roc_rf, col="blue")
legend("bottomright", legend=c("RF","NN","kNN"), col=c("blue","green","red"), lwd=2)

# Area under curves
auc_knn <- auc(roc_knn)
auc_nn <- auc(roc_nn)
auc_rf <- auc(roc_rf)

print(paste("AUC of K-Nearest Neighbors:", auc_knn))
print(paste("AUC of Neural Network:", auc_nn))
print(paste("AUC of Random Forest:", auc_rf))

# How each variable affects RandomForest Model
varImpPlot(rf_model)
```

```{r}
#| label: balancedAccuracy

knn_cm <- table(Predicted = knn_model, Actual = y_test)
nn_cm <- table(Predicted = pred_best, Actual = y_test_NN)
rf_cm <- table(Predcited = pred_best_RF, Actual = y_test_RF)

# True positives
TP_knn <- ifelse("1" %in% rownames(knn_cm) & "1" %in% colnames(knn_cm), 
                 knn_cm["1","1"], 0)
TP_nn <- ifelse("1" %in% rownames(nn_cm) & "1" %in% colnames(nn_cm), nn_cm["1","1"], 0)
TP_rf <- ifelse("1" %in% rownames(rf_cm) & "1" %in% colnames(rf_cm), rf_cm["1","1"], 0)

# False Positives
FP_knn <- ifelse("1" %in% rownames(knn_cm) & "0" %in% colnames(knn_cm), 
                 knn_cm["1","0"], 0)
FP_nn <- ifelse("1" %in% rownames(nn_cm) & "0" %in% colnames(nn_cm), nn_cm["1","0"], 0)
FP_rf <- ifelse("1" %in% rownames(rf_cm) & "0" %in% colnames(rf_cm), rf_cm["1","0"], 0)

# False Negatives
FN_knn <- ifelse("0" %in% rownames(knn_cm) & "1" %in% colnames(knn_cm), 
                 knn_cm["0","1"], 0)
FN_nn <- ifelse("0" %in% rownames(nn_cm) & "1" %in% colnames(nn_cm), nn_cm["0","1"], 0)
FN_rf <- ifelse("0" %in% rownames(rf_cm) & "1" %in% colnames(rf_cm), rf_cm["0","1"], 0)

# True Negatives
TN_knn <- ifelse("0" %in% rownames(knn_cm) & "0" %in% colnames(knn_cm), 
                 knn_cm["0","0"], 0)
TN_nn <- ifelse("0" %in% rownames(nn_cm) & "0" %in% colnames(nn_cm), nn_cm["0","0"], 0)
TN_rf <- ifelse("0" %in% rownames(rf_cm) & "0" %in% colnames(rf_cm), rf_cm["0","0"], 0)

# Recalls and Specificities
recall_knn <- ifelse((TP_knn + FN_knn) > 0, TP_knn/(TP_knn + FN_knn), 0)
recall_nn <- ifelse((TP_nn + FN_nn) > 0, TP_nn/(TP_nn + FN_nn), 0)
recall_rf <- ifelse((TP_rf + FN_rf) > 0, TP_rf/(TP_rf + FN_rf), 0)

spec_knn <- ifelse((TN_knn + FP_knn) > 0, TN_knn/(TN_knn + FP_knn), 0)
spec_nn <- ifelse((TN_nn + FP_nn) > 0, TN_nn/(TN_nn + FP_nn), 0)
spec_rf <- ifelse((TN_rf + FP_rf) > 0, TN_rf/(TN_rf + FP_rf), 0)

# Balanced Accuracies
BA_knn <- (recall_knn + spec_knn)/2
BA_nn <- (recall_nn + spec_nn)/2
BA_rf <- (recall_rf + spec_rf)/2
```

```{r}
#| label: evaluationMetrics

knn_metrics <- list(
  "F1 Score" = round(f1_vec[best_k], 3),
  Accuracy = round(acc_k3, 3),
  "Area Under Curve" = round(auc_knn, 3),
  "Balanced Accuracy" = round(BA_knn, 3)
)

nn_metrics <- list(
  "F1 Score" = round(f1_vec_NN[best_index], 3),
  Accuracy = round(acc_NN, 3),
  "Area Under Curve" = round(auc_nn, 3),
  "Balanced Accuracy" = round(BA_nn, 3)
)

rf_metrics <- list(
  "F1 Score" = round(f1_vec_RF[best_index_RF], 3),
  Accuracy = round(acc_RF, 3),
  "Area Under Curve" = round(auc_rf, 3),
  "Balanced Accuracy" = round(BA_rf, 3)
)

print("k-Nearest Neighbors")
for(i in seq_along(knn_metrics)){
  print(paste0(names(knn_metrics[i]),": ",knn_metrics[[i]]))
}

print("Neural Network")
for(i in seq_along(nn_metrics)){
  print(paste0(names(nn_metrics[i]),": ",nn_metrics[[i]]))
}

print("Random Forest")
for(i in seq_along(rf_metrics)){
  print(paste0(names(rf_metrics[i]),": ",rf_metrics[[i]]))
}
```

```{r}
#| label: predict2025

cbb25_test <- cbb25 %>%
  rename(
    "TPR" = `2P_O`,
    "TPRD" = `2P_D`
  ) %>%
  select(Team, all_of(features_RF)) 

probs_2025 <- predict(rf_model, 
                      newdata = cbb25_test, 
                      type = "prob")[,2]
pred_2025 <- ifelse(probs_2025 > thresholds[best_index_RF], 1, 0)

for(i in seq_along(probs_2025)){
  names(probs_2025) <- cbb25_test$Team
}

predictions <- data.frame(
  probs = probs_2025,
  preds = pred_2025,
  Team = cbb25_test$Team
) %>%
  arrange(desc(probs))

predictedFinalFours <- predictions %>%
  filter(preds == 1) %>%
  mutate(probs = round(probs,2)) %>%
  arrange(desc(probs))

top4Teams <- predictions %>%
  top_n(4, probs)

## Plot Top 4 teams and 4 honorable mentions
ggplot(
  data = predictedFinalFours,
  mapping = aes(x = reorder(Team, -probs), y = probs)
) +
  geom_col(color = "black", fill = "skyblue") +
  geom_text(aes(label = probs),
            vjust = -0.3, size = 3)+
  scale_y_continuous(limits = c(0,1)) +
  labs(title = "Teams and Their Probability to Make the Final Four",
       x = "Team",
       y = "Probability") +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", hjust = 0.5),
        axis.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1))
```


